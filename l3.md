If by “Level 3” you mean full-depth order-by-order (MBO) messages (each order has its own ID/price/size, add/modify/cancel/trade), you generally don’t want to treat it like “a more detailed Level 2.” You want to turn it into (a) a clean book state and (b) event/queue features that L2 can’t see.

Here’s the practical pipeline that won’t get you lost.

⸻

1) First decision: do you really need raw L3, or is it enough to reconstruct L2?

Most of the time:
	•	Use L3 to reconstruct a correct L2 (MBP10 snapshots) + compute “extra” queue features.
	•	Train on 1s/5s bars like you’re already doing, but with better signals.

If your end goal is short-horizon prediction/trading, L3’s biggest value is queue and trade attribution accuracy, not “more levels.”

⸻

2) Core state you must maintain (per instrument)

A) Order table

A map keyed by order_id:
\text{order\_id} \mapsto (\text{side}, p, q, t_{\text{add}}, \text{flags})

B) Price-level queues

For each price level p and side:
	•	Total visible size: Q(p)
	•	Order count: C(p)
	•	FIFO order list (optional but useful for queue-position features)

C) Best bid/ask and top-K ladder (derived)

Maintain an efficient structure (e.g., sorted dict / heap) so you can query top-10 at any time.

⸻

3) Apply events (the only correct way)

Each message updates the order table and aggregated price level. Conceptually:
	•	Add(order_id, side, p, q)
Q(p) \leftarrow Q(p)+q, C(p)\leftarrow C(p)+1
	•	Cancel(order_id, \Delta q) (partial) or remove
Q(p)\leftarrow Q(p)-\Delta q, remove if 0
	•	Modify(order_id, new p / new q)
If price changes: subtract from old level, add to new level.
If size changes: update Q(p) by delta.
	•	Trade
This is tricky: depending on feed, trade might come as
	•	separate trade print + size reduction on resting order(s), or
	•	explicit execution message against order IDs.
Best practice: treat trades as size removals from resting orders when order IDs are provided; otherwise infer via reductions.

⸻

4) What L3 gives you beyond L2 (worth extracting)

A) True limit order flow (adds vs cancels vs executed)

Instead of \Delta q at the top level (which mixes cancels and trades), you can separate:

Per bar [t,t+\Delta):
\text{AddVol}^b=\sum q_{\text{added, bid}},\quad
\text{CancelVol}^b=\sum q_{\text{canceled, bid}},\quad
\text{ExecVol}^b=\sum q_{\text{executed, bid}}
(and similarly for ask)

Then “flow imbalance” becomes much cleaner:
\text{OFI}^{add}_\Delta = \text{AddVol}^b-\text{AddVol}^a,\quad
\text{OFI}^{cancel}_\Delta = \text{CancelVol}^a-\text{CancelVol}^b
(signs depend on your convention; keep consistent)

B) Queue position / queue depletion risk (the killer feature set)

If you place a hypothetical order at best bid, you can estimate its queue position.

Let best bid price be b_1. Define:
Q^{ahead}_b(t)=\text{total resting size ahead of you at } b_1
Then define the depletion ratio over \Delta:
\text{DepleteRatio}_b(t,\Delta) = \frac{\text{RemovedVol at } b_1 \text{ over }\Delta}{Q_b(b_1,t)+\epsilon}
Meaning: probability you get “run over” or that best price flips.

You can compute removed volume by decomposing:
	•	executed against bid queue (sells hit bid),
	•	canceled from bid queue.

C) Order lifetime / toxicity

Compute average lifetime of canceled orders:
\text{LifeCancel}_\Delta = \operatorname{mean}(t_{\text{cancel}}-t_{\text{add}})
Short lifetimes + high cancel rates often signal adverse selection / spoofiness.

D) “Replace” behavior (modify patterns)

How often orders are modified rather than canceled/re-added:
\text{ModifyRate}_\Delta = \frac{N_{\text{modify}}}{N_{\text{events}}}
This can proxy participant style and regime.

⸻

5) Sampling L3 to 1s/5s (clean approach)

Do two streams:

Stream 1: L2 snapshots (derived from L3)

At bar end t_{i+1}, compute the top-10 (bid/ask px, sz, ct) and then compute your L2 features (spread, imbalance, microprice, depth, impact proxies).

Stream 2: L3 flow aggregates inside bar

Within [t_i,t_{i+1}), aggregate:
	•	add/cancel/exec volumes by side and by distance-from-mid buckets (touch vs 1–2 ticks away vs deeper)
	•	update counts
	•	queue depletion at best prices
	•	cancel-to-trade ratio

Then merge them on bar timestamp.

This is the “best of both worlds” setup: stable state features + informative event features.

⸻

6) Practical gotchas (L3 is where people bleed)
	•	Out-of-order / dropped messages: use sequence and flags to detect gaps; if gap, you should reset book from a snapshot or mark bars as invalid.
	•	Partial cancels vs full cancels: treat size deltas carefully; never allow negative level sizes.
	•	Trades without order IDs: you may need heuristic attribution; if the feed provides aggressor side, use it to assign which queue got hit.
	•	Hidden/iceberg: L3 only sees displayed portion; execution can exceed displayed in some venues—handle gracefully.
	•	Clearing events (“clear book”): hard reset state immediately.
	•	Performance: do per-instrument state, vectorize only the final bar features.

⸻

7) What I’d do for your project (minimal but strong)
	1.	Reconstruct MBP10 from L3 (or trust vendor MBP10 if it’s consistent).
	2.	Compute your existing L2 snapshot features (spread/imbalance/microprice/impact).
	3.	Add L3-only bar aggregates:
	•	add/cancel/exec volume imbalance at touch and near-touch
	•	cancel rate, trade rate, cancel-to-trade
	•	depletion ratio at best bid/ask
	•	mean cancel lifetime
	4.	Train on 1s and 5s horizons, compare incremental lift vs L2-only.

⸻

If you paste the L3 schema fields you have (esp. whether you get order_id, whether trades reference resting order IDs, and how “Trade” is represented), I can give you the exact state-update logic and a clean Python implementation that produces (a) MBP10 snapshots and (b) L3 flow features on 1s/5s bars.