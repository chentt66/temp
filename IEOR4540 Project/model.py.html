<html>
<head>
<title>model.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #000080; font-weight: bold;}
.s1 { color: #000000;}
.s2 { color: #0000ff;}
.s3 { color: #808080; font-style: italic;}
.s4 { color: #008000; font-weight: bold;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
model.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span>torch.nn <span class="s0">as </span>nn
<span class="s0">from </span>functools <span class="s0">import </span>partial
<span class="s0">import </span>torch.nn.functional <span class="s0">as </span>F
<span class="s0">import </span>math
<span class="s0">import </span>torch


<span class="s0">class </span>PatchEmbedding(nn.Module):
    <span class="s0">def </span>__init__(self, img_size=<span class="s2">224</span>, patch_size=<span class="s2">16</span>, in_chans=<span class="s2">3</span>, embed_dim=<span class="s2">768</span>):
        super().__init__()
        <span class="s3">#image_size=(image_size,image_size)</span>
        <span class="s3">#patch_size=(patch_size,patch_size)</span>
        self.img_size = img_size
        self.patch_size = patch_size

        self.n_patches = (img_size // patch_size) ** <span class="s2">2</span>
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

        self.cls_token = nn.Parameter(torch.randn(<span class="s2">1</span>, <span class="s2">1</span>, embed_dim))
        self.pos_embedding = nn.Parameter(torch.randn(<span class="s2">1</span>, self.n_patches + <span class="s2">1</span>, embed_dim))
        self.linear = nn.Linear(embed_dim, <span class="s2">768</span>)

        norm_layer = partial(nn.LayerNorm, eps=<span class="s2">1e-6</span>)
        self.norm = norm_layer(embed_dim)


    <span class="s0">def </span>forward(self, x):
        B, _, _, _ = x.shape
     
        x = self.proj(x) 

        x = x.flatten(<span class="s2">2</span>).transpose(<span class="s2">1</span>, <span class="s2">2</span>)  
        <span class="s3">#print(&quot;After flatten and transpose shape:&quot;, x.shape)</span>

        cls_token = self.cls_token.expand(B, -<span class="s2">1</span>, -<span class="s2">1</span>)  
        x = torch.cat((cls_token, x), dim=<span class="s2">1</span>) 
        <span class="s3">#print(&quot;After adding cls token shape:&quot;, x.shape)</span>

        x = x + self.pos_embedding 
        <span class="s3">#print(&quot;After adding positional embedding shape:&quot;, x.shape)</span>

        x = self.linear(x)
        x = self.norm(x)
        <span class="s0">return </span>x


<span class="s0">class </span>MultiHeadAttention(nn.Module):
    <span class="s0">def </span>__init__(self, embed_dim, num_heads=<span class="s2">12</span>, qkv_bias=<span class="s0">False</span>, atten_drop_ratio=<span class="s2">0.</span>, proj_drop_ratio=<span class="s2">0.</span>):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.head_dim = embed_dim // num_heads
        <span class="s0">assert </span>self.head_dim * num_heads == embed_dim, <span class="s4">&quot;embed_dim must be divisible by num_heads&quot;</span>

        self.scale = self.head_dim ** -<span class="s2">0.5</span>

        self.qkv = nn.Linear(embed_dim, embed_dim * <span class="s2">3</span>, bias=qkv_bias)
        self.atten_drop = nn.Dropout(atten_drop_ratio)
        self.proj = nn.Linear(embed_dim, embed_dim)
        self.proj_drop = nn.Dropout(proj_drop_ratio)

    <span class="s0">def </span>forward(self, x, mask=<span class="s0">None</span>):
        batch_size = x.size(<span class="s2">0</span>)
        qkv = self.qkv(x).reshape(batch_size, -<span class="s2">1</span>, <span class="s2">3</span>, self.num_heads, self.head_dim).permute(<span class="s2">2</span>, <span class="s2">0</span>, <span class="s2">3</span>, <span class="s2">1</span>, <span class="s2">4</span>)
        q, k, v = qkv[<span class="s2">0</span>], qkv[<span class="s2">1</span>], qkv[<span class="s2">2</span>]  

        <span class="s3"># Scaled Dot-Product Attention</span>
        attn = (q @ k.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>)) * self.scale
        <span class="s0">if </span>mask <span class="s0">is not None</span>:
            attn = attn.masked_fill(mask == <span class="s2">0</span>, float(<span class="s4">'-inf'</span>))
        attn = F.softmax(attn, dim=-<span class="s2">1</span>)
        attn = self.atten_drop(attn)

        weighted_avg = attn @ v  
        weighted_avg = weighted_avg.transpose(<span class="s2">1</span>, <span class="s2">2</span>).reshape(batch_size, -<span class="s2">1</span>, self.embed_dim)

        x = self.proj(weighted_avg)
        x = self.proj_drop(x)
        <span class="s0">return </span>x



<span class="s0">class </span>MLP(nn.Module):
    <span class="s0">def </span>__init__(self, in_features, hidden_features, out_features=<span class="s0">None</span>, dropout=<span class="s2">0.</span>):
        super(MLP, self).__init__()

        out_features = out_features <span class="s0">or </span>in_features

        self.norm = nn.LayerNorm(in_features)  
        self.fc1 = nn.Linear(in_features, hidden_features)  
        self.act = nn.GELU()  
        self.drop = nn.Dropout(dropout) 
        self.fc2 = nn.Linear(hidden_features, out_features)  

    <span class="s0">def </span>forward(self, inputs):
        x = self.norm(inputs)
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)

        x = self.fc2(x)
        x = self.drop(x)

        <span class="s0">return </span>x


<span class="s0">class </span>Encoder(nn.Module):
    <span class="s0">def </span>__init__(self, embed_dim, num_heads, mlp_ratio=<span class="s2">4.</span>, dropout_rate=<span class="s2">0.</span>):
        super(Encoder, self).__init__()

        self.norm1 = nn.LayerNorm(embed_dim)
        self.attention = MultiHeadAttention(embed_dim, num_heads)
        self.drop1 = nn.Dropout(dropout_rate)

        self.norm2 = nn.LayerNorm(embed_dim)
        hidden_features = int(embed_dim * mlp_ratio)
        self.mlp = MLP(embed_dim, hidden_features, dropout=dropout_rate)
        self.drop2 = nn.Dropout(dropout_rate)

    <span class="s0">def </span>forward(self, x):
        x = x + self.drop1(self.attention(self.norm1(x)))
        x = x + self.drop2(self.mlp(self.norm2(x)))

        <span class="s0">return </span>x


<span class="s0">class </span>ViT(nn.Module):
    <span class="s0">def </span>__init__(self, num_classes=<span class="s2">1000</span>, depth=<span class="s2">12</span>, drop_ratio=<span class="s2">0.</span>, embed_dim=<span class="s2">768</span>, img_size=<span class="s2">224</span>, patch_size=<span class="s2">16</span>, in_chans=<span class="s2">3</span>):
        super(ViT, self).__init__()

        self.patchembed = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        self.blocks = nn.Sequential(*[Encoder(embed_dim, num_heads=<span class="s2">12</span>, mlp_ratio=<span class="s2">4.</span>, dropout_rate=drop_ratio) <span class="s0">for </span>_ <span class="s0">in </span>range(depth)])
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

        self._init_weights()

    <span class="s0">def </span>_init_weights(self):
        <span class="s0">for </span>m <span class="s0">in </span>self.modules():
            <span class="s0">if </span>isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=<span class="s2">0.02</span>)
                <span class="s0">if </span>m.bias <span class="s0">is not None</span>:
                    nn.init.zeros_(m.bias)
            <span class="s0">elif </span>isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    <span class="s0">def </span>forward(self, x):
        <span class="s3">#print(&quot;Input shape:&quot;, x.shape)</span>
        x = self.patchembed(x)  
        <span class="s3">#print(&quot;After patch embedding shape:&quot;, x.shape)</span>

        x = self.blocks(x)   
        <span class="s3">#print(&quot;After encoder shape:&quot;, x.shape)</span>
        x = self.norm(x)       
        <span class="s3">#print(&quot;After norm shape:&quot;, x.shape)</span>

        x = self.head(x[:, <span class="s2">0</span>])
        <span class="s3">#print(&quot;Final shape:&quot;, x.shape)</span>

        <span class="s0">return </span>x


<span class="s0">class </span>MultiHeadLinformerAttention(nn.Module):
    <span class="s0">def </span>__init__(self, embed_dim, seq_len, k, num_heads=<span class="s2">12</span>, qkv_bias=<span class="s0">False</span>, atten_drop_ratio=<span class="s2">0.</span>, proj_drop_ratio=<span class="s2">0.</span>):
        super(MultiHeadLinformerAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        <span class="s0">assert </span>self.head_dim * num_heads == embed_dim, <span class="s4">&quot;embed_dim must be divisible by num_heads&quot;</span>

        self.qkv = nn.Linear(embed_dim, <span class="s2">3 </span>* embed_dim, bias=qkv_bias)
        self.scale = <span class="s2">1.0 </span>/ math.sqrt(self.head_dim)
        self.atten_drop = nn.Dropout(atten_drop_ratio)
        self.proj_drop = nn.Dropout(proj_drop_ratio)
        self.k = k

        <span class="s3"># Linformer projections for keys and values</span>
        self.key_proj = nn.Linear(seq_len, k)
        self.value_proj = nn.Linear(seq_len, k)

 
        self.proj = nn.Linear(embed_dim, embed_dim)


    <span class="s0">def </span>forward(self, x, mask=<span class="s0">None</span>):
        batch_size, seq_length, _ = x.size()
        qkv = self.qkv(x).reshape(batch_size, seq_length, <span class="s2">3</span>, self.num_heads, self.head_dim).permute(<span class="s2">2</span>, <span class="s2">0</span>, <span class="s2">3</span>, <span class="s2">1</span>, <span class="s2">4</span>)
        q, k, v = qkv[<span class="s2">0</span>], qkv[<span class="s2">1</span>], qkv[<span class="s2">2</span>]  

        <span class="s0">if </span>self.k <span class="s0">is not None</span>:
            k = k.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>)  
            v = v.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>)  
            k = self.key_proj(k)
            v = self.value_proj(v)
            k = k.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>) 
            v = v.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>)  

        <span class="s0">if </span>mask <span class="s0">is not None</span>:
            mask = mask[:, <span class="s0">None</span>, <span class="s0">None</span>, :self.k].expand(-<span class="s2">1</span>, self.num_heads, -<span class="s2">1</span>, -<span class="s2">1</span>)
            k = k.masked_fill(~mask, <span class="s2">0.0</span>)
            v = v.masked_fill(~mask, <span class="s2">0.0</span>)

        q = q * self.scale

        qk = torch.matmul(q, k.transpose(-<span class="s2">2</span>, -<span class="s2">1</span>))

        <span class="s0">if </span>mask <span class="s0">is not None</span>:
            qk = qk.masked_fill(~mask, float(<span class="s4">'-inf'</span>))

        attn = F.softmax(qk, dim=-<span class="s2">1</span>)
        attn = self.atten_drop(attn)

        weighted_avg = torch.matmul(attn, v)
        weighted_avg = weighted_avg.transpose(<span class="s2">2</span>, <span class="s2">3</span>).reshape(batch_size, -<span class="s2">1</span>, self.num_heads * self.head_dim)

        x = self.proj(weighted_avg)
        x = self.proj_drop(x)
        <span class="s0">return </span>x


<span class="s0">class </span>LinEncoder(nn.Module):
    <span class="s0">def </span>__init__(self, embed_dim, num_heads, seq_len, k, mlp_ratio=<span class="s2">4.</span>, dropout_rate=<span class="s2">0.</span>):
        super(LinEncoder, self).__init__()

        self.norm1 = nn.LayerNorm(embed_dim)
        self.attention = MultiHeadLinformerAttention(embed_dim, seq_len, k, num_heads)
        self.drop1 = nn.Dropout(dropout_rate)

        self.norm2 = nn.LayerNorm(embed_dim)
        hidden_features = int(embed_dim * mlp_ratio)
        self.mlp = MLP(embed_dim, hidden_features, dropout=dropout_rate)
        self.drop2 = nn.Dropout(dropout_rate)

    <span class="s0">def </span>forward(self, x, mask=<span class="s0">None</span>):

        x = x + self.drop1(self.attention(self.norm1(x), mask))
        x = x + self.drop2(self.mlp(self.norm2(x)))

        <span class="s0">return </span>x


<span class="s0">class </span>LinViT(nn.Module):
    <span class="s0">def </span>__init__(self, num_classes=<span class="s2">1000</span>, depth=<span class="s2">12</span>, drop_ratio=<span class="s2">0.</span>, embed_dim=<span class="s2">768</span>, img_size=<span class="s2">224</span>, patch_size=<span class="s2">16</span>,
                 in_chans=<span class="s2">3</span>, linformer_k=<span class="s2">128</span>):
        super(LinViT, self).__init__()
        self.patchembed = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)

        self.pos_drop = nn.Dropout(drop_ratio)
        self.blocks = nn.Sequential(*[LinEncoder(embed_dim, num_heads=<span class="s2">12</span>, seq_len=self.patchembed.n_patches + <span class="s2">1</span>, k=linformer_k,
                                                 mlp_ratio=<span class="s2">4.</span>, dropout_rate=drop_ratio) <span class="s0">for </span>_ <span class="s0">in </span>range(depth)])
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, num_classes)

        self._init_weights()

    <span class="s0">def </span>_init_weights(self):
        <span class="s0">for </span>m <span class="s0">in </span>self.modules():
            <span class="s0">if </span>isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, std=<span class="s2">0.02</span>)
                <span class="s0">if </span>m.bias <span class="s0">is not None</span>:
                    nn.init.zeros_(m.bias)
            <span class="s0">elif </span>isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    <span class="s0">def </span>forward(self, x, mask=<span class="s0">None</span>):
        x = self.patchembed(x) 
        x = self.pos_drop(x)
        x = self.blocks(x)
        x = self.norm(x)
        x = self.head(x[:, <span class="s2">0</span>]) 
        <span class="s0">return </span>x
</pre>
</body>
</html>